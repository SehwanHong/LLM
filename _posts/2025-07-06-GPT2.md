---
title: "Language Model is Unsupervised Multitask Learner (GPT-2): Paper Review"
date: 2025-07-07
layout: default   # 또는 page, default 등 테마에 따라
tags: [llm, paper, gpt, gpt-2]
---

## Introduction

2019년 2월, OpenAI는 "Language Models are Unsupervised Multitask Learners"라는 도발적인 제목의 논문을 발표하며 전 세계 AI 커뮤니티에 다시 한번 큰 충격을 안겼습니다. 이 논문은 GPT-1의 후속 모델인 GPT-2를 세상에 공개하며, 단순히 모델의 크기를 키우고 더 많은, 그리고 더 양질의 데이터로 학습시키는 것만으로도 언어 모델이 별도의 지도 학습(fine-tuning) 없이 다양한 종류의 과업을 수행할 수 있는 '다중 작업 학습자(Multitask Learner)'가 될 수 있음을 증명했습니다.

GPT-1이 **'사전 훈련 및 파인 튜닝'** 패러다임을 정립했다면, GPT-2는 여기서 한 걸음 더 나아가 **'제로샷(Zero-shot)'** 학습의 가능성을 활짝 열었습니다. 즉, 특정 작업에 대한 단 하나의 예제도 보여주지 않고, 오직 자연어 지시(prompt)만으로 모델이 해당 과업을 이해하고 수행하게 만든 것입니다. 이는 범용 인공지능(AGI)을 향한 중요한 이정표로 평가받으며, 이후 거대 언어 모델(LLM) 경쟁의 기폭제가 되었습니다.

## GPT-2

GPT-2는 GPT-1과 모델 구조적으로 거의 동일하다. 다만 GPT-2에서 이야기하고 싶어하는 것은 모델과 데이터 셋의 크기를 극단적으로 키우면, 언어 모델링이라는 단일 목표만으로도 다양한 능력이 저절로 창발(emerge)한다는 것이다.

### 언어 모델링의 재해석: 모든 NLP 과업은 언어 모델링의 일부

본 논문은 기존 NLP의 패러다임을 근본적으로 재해석합니다. 전통적으로 번역, 요약, 질문 응답 등은 각기 다른 형식의 입출력(p(output∣input))을 가지는 별개의 문제로 취급되었습니다. 하지만 저자들은 이 모든 과업이 사실상 조건부 확률을 구하는 하나의 거대한 언어 모델링 문제(p(output∣input,task))로 통합될 수 있다고 주장합니다.

예를 들어,

- 번역: (translate to french, english text, french text) 형식의 텍스트를 학습한 모델에게 translate to french, a dog, 라는 프롬프트를 주면 un chien을 예측하도록 할 수 있습니다.

- 질문 응답: (answer the question, document, question, answer) 형식의 텍스트를 학습했다면, answer the question, <장문의 글>, <질문>, 이라는 프롬프트에 대한 답을 생성할 수 있습니다.

문제는 이러한 다양한 형식의 데이터를 명시적으로 구축하기 어렵다는 것입니다. GPT-2는 여기서 발상의 전환을 합니다. 인터넷처럼 방대하고 다양한 비정형 텍스트 데이터에는 이미 이러한 과업들이 암묵적으로 내재되어 있으며, 매우 큰 용량의 언어 모델은 데이터에 담긴 이러한 패턴들을 스스로 학습하여 별도의 지시 없이도 다중 작업 능력을 갖추게 된다는 가설을 세웁니다.

### 모델과 데이터의 극적인 확장

이 가설을 증명하기 위해 OpenAI는 세 가지 요소를 대폭 확장했습니다.

- 데이터셋 (WebText): 기존 연구들이 사용하던 데이터셋(예: 뉴스, 위키피디아)이 특정 도메인에 편중되어 있다고 판단하고, 더 범용적인 언어 능력을 학습시키기 위해 새로운 데이터셋인 **'WebText'**를 구축했습니다. 이는 소셜 미디어 플랫폼인 Reddit에서 3개 이상의 추천(karma)을 받은 아웃바운드 링크만을 수집하여 정제한, 약 40GB 크기의 고품질 영문 텍스트 데이터셋입니다. 이는 특정 주제에 치우치지 않고, 인간이 흥미롭다고 판단한 다양한 종류의 글을 포함하고 있습니다.

- 모델 파라미터: GPT-1이 1억 1,700만 개의 파라미터를 가졌던 것에 비해, GPT-2는 가장 큰 모델 기준으로 15억 개(1.5B)의 파라미터를 가지도록 설계되었습니다. 이는 약 13배 증가한 수치로, 모델의 표현력과 학습 용량을 비약적으로 향상시켰습니다. (논문에서는 117M, 345M, 762M, 1.5B 네 가지 크기의 모델을 실험합니다.)

- 아키텍처 개선: 기본적인 구조는 GPT-1과 같은 트랜스포머 디코더 스택이지만, 몇 가지 소소한 개선이 이루어졌습니다. Layer Normalization의 위치를 조정하고, 모델 깊이에 따라 가중치 초기화를 변경하는 등 안정적인 학습을 위한 장치들이 추가되었습니다.

## 의의 및 영향: 윤리적 논쟁과 LLM 시대의 개막

GPT-2 논문과 모델은 기술적 성취만큼이나 사회적으로 큰 파장을 일으켰습니다.

- 악용 가능성에 대한 경고와 단계적 모델 공개: OpenAI는 GPT-2의 텍스트 생성 능력이 가짜 뉴스, 스팸, 피싱 등 악의적인 목적으로 사용될 위험이 크다고 판단하여, 처음에는 가장 큰 1.5B 모델을 즉시 공개하지 않고 작은 모델부터 순차적으로 공개하는 '단계적 공개(staged release)' 정책을 펼쳤습니다. 이는 AI 연구의 사회적 책임과 윤리적 문제를 공론화하는 중요한 계기가 되었습니다.

- 거대 언어 모델(LLM) 경쟁의 서막: GPT-2의 성공은 '스케일'이 곧 '성능'으로 이어진다는 것을 명확히 보여주었습니다. 이후 구글, 메타 등 빅테크 기업들은 앞다투어 GPT-2보다 훨씬 더 큰 규모의 언어 모델 개발에 뛰어들었고, 이는 현재의 초거대 AI 경쟁 시대를 열었습니다.

- 프롬프트 엔지니어링의 부상: 모델을 파인 튜닝하는 대신, 원하는 결과를 얻기 위해 입력값(프롬프트)을 어떻게 구성하고 디자인할 것인가에 대한 고민, 즉 **'프롬프트 엔지니어링(Prompt Engineering)'**의 중요성이 처음으로 대두되었습니다.

## 참고자료

[https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)